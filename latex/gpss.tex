\documentclass[twoside]{article}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amssymb,amsmath,amsthm}
\usepackage{graphicx}
\usepackage{preamble}
\usepackage{natbib}
\usepackage{hyperref}
\usepackage{color}
\usepackage{wasysym}
\usepackage{subfigure}
\usepackage{bm}
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\definecolor{mydarkblue}{rgb}{0,0.08,0.45}
\hypersetup{ %
    pdftitle={},
    pdfauthor={},
    pdfsubject={},
    pdfkeywords={},
    pdfborder=0 0 0,
    pdfpagemode=UseNone,
    colorlinks=true,
    linkcolor=mydarkblue,
    citecolor=mydarkblue,
    filecolor=mydarkblue,
    urlcolor=mydarkblue,
    pdfview=FitH}
    
\input{include/commenting.tex}
\usepackage{format/icml2013}


%% For submission, make all render blank.
%\renewcommand{\LATER}[1]{}
%\renewcommand{\fLATER}[1]{}
%\renewcommand{\TBD}[1]{}
%\renewcommand{\fTBD}[1]{}
%\renewcommand{\PROBLEM}[1]{}
%\renewcommand{\fPROBLEM}[1]{}
%\renewcommand{\NA}[1]{#1}  %% Note, NA's pass through!
    
%\subsection{How to use commenting}
%\NA{Needs attention.}
%\TBD{To be done.}
%\PROBLEM{Problem.}
%\fTBD{Margin to be done.}
%\fPROBLEM{Margin problem.}
    
\begin{document}

\twocolumn[
\icmltitle{Structure Discovery through Kernel Search}
%\icmlauthor{Anonymous Authors}
%\icmladdress{ Unknown Institution}
\icmlkeywords{nonparametrics, gaussian process, machine learning, ICML}
\vskip 0.3in
]


\begin{abstract}
Gaussian process (GP) models are used widely and successfully.  However, their effictiveness depends critically on choosing an appropriate family of kernels.  This aspect of GP modeling has been sorely underdeveloped.  In this paper, we introduce a procedure for automatically and efficiently searching through a large space of GP models.
\end{abstract}

\section{Introduction}

Similar searches over large model classes have been succesfully used in machine vision [cite Cox + Pinto].  In general, learning the model class from data seems superior proposing the model beforehand.  In high dimensional problems, it is also hard for a practitioner to propose an appropriate model even after examining a dataset closely.  Choosing a kernel family is also a stumbling block for non-experts who wish to use Gaussian Process models.

[Zoubin says:  There has been tons of work done on automatic structure discovery in unsupervised settings, but little done in supervised learning.]

\section{Gaussian Processes Priors}

Gaussian processes are a flexible and tractable prior over functions, useful for solving regression and classification tasks\cite{rasmussen38gaussian}.  The kind of structure which can be captured by a GP model is mainly determined by its \emph{kernel}: the covariance function.  One of the main difficulties in specifying a Gaussian process model is in choosing a kernel which can represent the structure present in the data.  For small to medium-sized datasets, the kernel has a large impact on modeling efficacy.

\NA{Note: The above paragraph is plagarized from my additive GP paper. -David} 

[TODO: add GP math intro]

\section{Expressing Structure through Kernels}

[Emphasize that these techniques could work on SVMs, but that this would be difficult because model selection is hard without evidence measures]

[Explain exactly how one can express structure on functions, with examples]

Examples: Local linear structure, local periodicity.

Figure \ref{fig:kernels} shows how to construct such structures as:  Long-range trends with local variation, local periodicity, long-range trends 

\input{tables/simple_kernels_table.tex}

\input{tables/example_structures_table.tex}


\section{Searching over structures}

[Explain how we search over structures.  Talk about marginal likelihood].

\section{Related Work}

Compositional Model search for unsupervsied learning: \cite{grosse2012exploiting}

Hyperkernels \cite{ong2002hyperkernels}

\paragraph{ANOVA Kernels}

Support vector regression with ANOVA decomposition kernels \cite{stitson1999support}

%\subsubsection{Smoothing spline ANOVA models}
A closely related procedure from the statistics literature is smoothing-splines ANOVA (SS-ANOVA)\cite{wahba1990spline, gu2002smoothing}. An SS-ANOVA model is estimated as a weighted sum of splines along each dimension, plus a sum of splines over all pairs of dimensions, all triplets, etc, with each individual interaction term having a separate weighting parameter.  Because the number of terms to consider grows exponentially in the order, in practice, only terms of first and second order are usually considered.  Learning in SS-ANOVA is usually done via penalized-maximum likelihood with a fixed sparsity hyperparameter.

\paragraph{Hierarchical Kernel Learning}

In "High-Dimensional Non-Linear Variable Selection through Hierarchical Kernel Learning", Bach\cite{DBLP:journals/corr/abs-0909-0844} uses a regularized optimization framework to learn a weighted sum over an exponential number of kernels which can be computed in polynomial time.  The subsets of kernels considered by this method are restricted to be a \textit{hull} of kernels.\footnote{In the setting we are considering in this paper, a hull can be defined as a subset of all terms such that if term $\prod_{j \in J} k_j(\bf x, x')$ is included in the subset, then so are all terms $\prod_{j \in J / i} k_j(\bf x, x')$, for all $i \in J$.  For details, see \cite{DBLP:journals/corr/abs-0909-0844}.}
Given each dimension's kernel, and a pre-defined weighting over all terms, HKL performs model selection by searching over hulls of interaction terms.
% \subsubsection{All-subsets kernel with uniform weightings}
In \cite{DBLP:journals/corr/abs-0909-0844}, Bach also fixes the relative weighting between orders of interaction with a single term $\alpha$, computing the sum over all orders by:
\begin{equation}
\label{eqn:uniform}
k_{a}({\bf x, x'}) = v_D^2 \prod_{d=1}^D \left(1 + \alpha k_{d}(x_{d}, x_{d}') \right)
\end{equation}
which has computational complexity $O(D)$.  However, this formulation forces the weight of all $n$th order terms to be weighted by $\alpha^n$.

The main difficulty with the approach of \cite{DBLP:journals/corr/abs-0909-0844} is that hyperparameters are hard to set other than by cross-validation.  In contrast, our method optimizes the hyperparameters of each dimension's base kernel, as well as the relative weighting of each order of interaction. 

Accuracy versus interpretability in flexible modeling: Implementing a tradeoff using Gaussian process models \cite{plate1999accuracy}

A related functional ANOVA GP model\cite{kaufman2010bayesian} decomposes the \emph{mean} function into a weighted sum of GPs. However, the effect of a particular degree of interaction cannot be quantified by that approach. Also, computationally, the Gibbs sampling approach used in \cite{kaufman2010bayesian} is disadvantageous.

\paragraph{Genetic Searches}

Evolving kernel functions for SVMs by genetic programming: \cite{diosan2007evolving}

A Genetic Programming based kernel construction and optimization method for Relevance Vector Machines: \cite{bing2010gp}

\paragraph{Equation Learning}

Equation discovery with ecological applications \cite{dzeroski1999equation}

Discovering admissible model equations from observed data based on scale-types and identity constraints \cite{washio1999discovering}

\paragraph{Multiple Kernel Learning}

\cite{christoudias2009bayesian} previously showed how mixtures of kernels can be learnt by gradient descent in the Gaussian process framework.  They call this \emph{Bayesian localized multiple kernel learning}.


\section{Experiments}

\paragraph{Kernels used}
%
Table \ref{tbl:kernel_descriptions} describes the base kernels used in our search.
%
\input{"tables/kernel_descriptions.tex"}

%\subsection{Real datasets}

\subsection{Mauna Loa Atmospheric Carbon Dioxide}

As an example of a GP modeling problem where choosing an appropriate structure is critical, we revisit a dataset explored in \cite{rasmussen38gaussian}, pages 120-126, where a kernel was hand-tailored to fit a GP model to the dataset.

%\begin{figure}
%\includegraphics[width=\columnwidth]{../figures/carls_kernel}
%\caption{Carl's kernel function on the Mauna dataset.}
%\end{figure}

%\begin{figure}
%\includegraphics[width=\columnwidth]{../figures/our_kernel}
%\caption{Our best kernel function on the Mauna dataset.}
%\end{figure}

\subsection{Decomposing the Signal}

A benefit of the GP framework is that, when a function is modeled with additive structure, we can decompose the signal into a sum of functions, and examine each individual component of our signal.

The conditional distribution of a Gaussian $\vf_1$ conditioned on its sum with another Gaussian $\vf = \vf_1 + \vf_2$ where $\vf_1 \sim \gp( \vmu_1, k_1)$ and $\vf_2 \sim \gp( \vmu_2, k_2)$ is given by
\begin{align}
\vf_1^\star | \vf \sim \mathcal{N} \big( & \vmu_1^\star + \vk_1^\star (\vK_1 + \vK_2)\inv \left( \vf - \vmu_1 - \vmu_2 \right), \nonumber \\
& \vk_1^{\star\star} - \vk_1^\star (\vK_1 + \vK_2)\inv \vk_1^\star \big).
\end{align}
and the covariance between the two components, conditioned on their sum is given by:
\begin{align}
\cov(\vf_1^\star, \vf_2^\star) | \vf = \vk_1^{\star\tra} (\vK_1 + \vK_2)\inv \vk_2^\star
\end{align}
Derivations of these formulae are given in the supplementary material.

These distributions express the posterior model uncertainty about different components of the signal, integrating over the possible configurations of the other components.

\TBD{Check that the notation for stars and kernel matrices is explained somewhere.}
\TBD{Smooth the flow of the previous subsection.}

Figure \ref{fig:mauna_all} shows the complete posterior of a $\gp$ regression model of the Mauna Loa dataset.  Figure \ref{fig:mauna_decomps} shows the same posterior signal decomposed into long-term, periodic, medium-term and short-term components, respectively.


\begin{figure*}
%\begin{minipage}{2\columnwidth}
%\begin{figure*}
\begin{minipage}{\columnwidth}
\includegraphics[width=\columnwidth]{../figures/decomposition/mauna_test_all}
\captionof{figure}{The complete posterior on the Mauna dataset.}
\label{fig:mauna_all}
\end{minipage}
%
\begin{minipage}{\columnwidth}
\centering
\newcommand{\fwmauna}{4cm}
\begin{tabular}{cc}
 \includegraphics[width=\fwmauna]{../figures/decomposition/mauna_test_1} & 
 \includegraphics[width=\fwmauna]{../figures/decomposition/mauna_test_2} \\
 \includegraphics[width=\fwmauna]{../figures/decomposition/mauna_test_3} & 
 \includegraphics[width=\fwmauna]{../figures/decomposition/mauna_test_4} 
\end{tabular}
\caption{Automatic decomposition of Mauna Loa data.}
\end{minipage}
%\end{minipage}
\end{figure*}
%\label{fig:mauna_decomps}
%\end{figure*}



\subsection{Synthetic experiments}

\subsubsection{Bach Synthetic Dataset}
In addition to standard UCI repository datasets, we generated a synthetic dataset following the same recipe as \cite{DBLP:journals/corr/abs-0909-0844}: From a covariance matrix drawn from a Wishart distribution with 1024 degrees of freedom, we select 8 variables.  We then construct the non-linear function $f(X) = \sum_{i=1}^4 \sum_{j=1+1}^4 X_i X_j + \epsilon$, which sums all 2-way products of the first 4 variables, and adds Gaussian noise $\epsilon$.  This dataset is one which can be predicted well by a kernel which is a sum of two-way interactions over the first 4 variables, ignoring the extra 4 noisy copies.

This dataset was designed by \cite{DBLP:journals/corr/abs-0909-0844} to demonstrate the advantages of HKL over GP-ARD. 

If the dataset is large enough, HKL can construct a hull around only those subsets of cross terms that are optimal for predicting the output.  GP-ARD, in contrast, can only learn to ignore the noisy copy variables, but cannot learn to ignore the higher-term interactions between the predictive variables.  However, a GP with an additive kernel can learn both to ignore irrelevant variables, and to ignore certain orders of interaction.  In this example, the additive GP is able to recover the relevant structure.

\subsection{Methods}

\paragraph{Structure Search}
All of the experiments in this paper were performed using the standard GPML toolbox\footnote{Available at \texttt{http://www.gaussianprocess.org/gpml/code/}}; code to perform all experiments is available at the authors' website.

\paragraph{Hierarchical Kernel Learning}	
HKL\footnote{Code for HKL available at \texttt{http://www.di.ens.fr/\textasciitilde fbach/hkl/}} was run using the all-subsets kernel, which corresponds to the same set of kernels as considered by the additive GP with a squared-exp base kernel.

%\input{"tables/kernels.tex"}
%\input{"tables/kernels2.tex"}
%\input{"tables/kernels3.tex"}
%\input{"tables/kernels4.tex"}
%\input{"tables/kernels5.tex"}

\paragraph{Additive Gaussian Processes} \cite{duvenaud2011additive11}

\subsection{Numerical evaluation of search method}

In addition to demonstrating the interpretability of our method we have compared the predictive performance of our method against other state of the art GP kernel search methods.

In particular we have extended the comparison of \cite{duvenaud2011additive11} to include our method.
5 data sets were split into 10 equally sized folds.
For each fold of each data set we trained each method on the other 9 folds, and then computed predictive mean squared error (MSE) and mean negative predictive log likelihood on the held out fold.
Average statistics for these experiments are presented in tables \ref{table:MSE} and \ref{table:nll}.

\input{tables/regression_results_mse.tex}
\input{tables/regression_results_nll.tex}

Our method outperforms the other methods in \emph{all} tests (but not statistically significantly yet).
Some points for discussion
\begin{itemize}
\item Experiments just using SE kernel can outperform additive kernel surprisingly. This is presumably a regularisation effect of using a finite depth search and/or BIC. We could make this a more Bayesian result (i.e.~more a property of the model) by placing a prior on kernels that depends on the number of components.
\item Need to discuss design choices e.g.~$k$ (i.e.~$k$ best kernels expanded), depth of search, base kernels.
\item \ldots
\end{itemize}

\section{Discussion}

Machine learning can be more data-driven, analogous to the high-thoughput approaches being used in biology. 

\section{Future Work}
\subsection{Bayesian Optimization}

cite Ryan Adams, Lizotte, de Frietas

\subsection{A Kernel between kernels}

Hyperkernels \cite{ong2002hyperkernels}
\section{Model-based search over models}

Bayesian optimization for hyper-parameter search: \cite{snoek2012practical}

\section{Conclusion}

\subsubsection*{Acknowledgements}


\bibliographystyle{format/icml2013}
\bibliography{gpss}
\end{document}
