\documentclass[twoside]{article}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amssymb,amsmath,amsthm}
\usepackage{graphicx}
\usepackage{preamble}
\usepackage{natbib}
\usepackage{hyperref}
\usepackage{color}
\usepackage{wasysym}
\usepackage{subfigure}
\usepackage{bm}
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\definecolor{mydarkblue}{rgb}{0,0.08,0.45}
\hypersetup{ %
    pdftitle={},
    pdfauthor={},
    pdfsubject={},
    pdfkeywords={},
    pdfborder=0 0 0,
    pdfpagemode=UseNone,
    colorlinks=true,
    linkcolor=mydarkblue,
    citecolor=mydarkblue,
    filecolor=mydarkblue,
    urlcolor=mydarkblue,
    pdfview=FitH}
    
\input{include/commenting.tex}
\usepackage{format/icml2013}

%% For submission, make all render blank.
%\renewcommand{\LATER}[1]{}
%\renewcommand{\fLATER}[1]{}
%\renewcommand{\TBD}[1]{}
%\renewcommand{\fTBD}[1]{}
%\renewcommand{\PROBLEM}[1]{}
%\renewcommand{\fPROBLEM}[1]{}
%\renewcommand{\NA}[1]{#1}  %% Note, NA's pass through!
    
    
% If your paper is accepted, change the options for the package
% aistats2e as follows:
%
%\usepackage[accepted]{aistats2e}
%
% This option will print headings for the title of your paper and
% headings for the authors names, plus a copyright note at the end of
% the first column of the first page.


\begin{document}

% If your paper is accepted and the title of your paper is very long,
% the style will print as headings an error message. Use the following
% command to supply a shorter title of your paper so that it can be
% used as headings.
%
%\runningtitle{I use this title instead because the last one was very long}

% If your paper is accepted and the number of authors is large, the
% style will print as headings an error message. Use the following
% command to supply a shorter version of the authors names so that
% they can be used as headings (for example, use only the surnames)
%
%\runningauthor{Surname 1, Surname 2, Surname 3, ...., Surname n}

\twocolumn[
\icmltitle{Kernel Structure Discovery in Gaussian Process Models}
%\icmlauthor{Anonymous Authors}
%\icmladdress{ Unknown Institution}
\icmlkeywords{nonparametrics, gaussian process, machine learning, ICML}
\vskip 0.3in
]


\begin{abstract}
Gaussian process (GP) models are used widely and successfully.  However, their effictiveness depends critically on choosing an appropriate family of kernels.  This aspect of GP modeling has been sorely underdeveloped.  In this paper, we introduce a procedure for automatically and efficiently searching through a large space of GP models.
\end{abstract}

\section{Introduction}

Similar searches over large model classes have been succesfully used in machine vision [cite Cox + Pinto].  In general, learning the model class from data seems superior proposing the model beforehand.  In high dimensional problems, it is also hard for a practitioner to propose an appropriate model even after examining a dataset closely.  Choosing a kernel family is also a stumbling block for non-experts who wish to use Gaussian Process models.

\subsection{How to use commenting}

\NA{Needs attention.}
\TBD{To be done.}
\PROBLEM{Problem.}
\fTBD{Margin to be done.}
\fPROBLEM{Margin problem.}

\section{GP Structure}

[Standard GP intro \cite{rasmussen38gaussian}]


\section{Related Work}

Compositional Model search for unsupervsied learning: \cite{grosse2012exploiting}

Hyperkernels \cite{ong2002hyperkernels}

\subsection{ANOVA Kernels}

Additive Gaussian Processes \cite{duvenaud2011additive11}

Support vector regression with ANOVA decomposition kernels \cite{stitson1999support}

\subsubsection{Smoothing spline ANOVA models}

A closely related procedure from the statistics literature is smoothing-splines ANOVA (SS-ANOVA)\cite{wahba1990spline, gu2002smoothing}. An SS-ANOVA model is estimated as a weighted sum of splines along each dimension, plus a sum of splines over all pairs of dimensions, all triplets, etc, with each individual interaction term having a separate weighting parameter.  Because the number of terms to consider grows exponentially in the order, in practice, only terms of first and second order are usually considered.  Learning in SS-ANOVA is usually done via penalized-maximum likelihood with a fixed sparsity hyperparameter.

\subsection{Hierarchical Kernel Learning}

In "High-Dimensional Non-Linear Variable Selection through Hierarchical Kernel Learning", Bach\cite{DBLP:journals/corr/abs-0909-0844} uses a regularized optimization framework to learn a weighted sum over an exponential number of kernels which can be computed in polynomial time.  The subsets of kernels considered by this method are restricted to be a \textit{hull} of kernels.\footnote{In the setting we are considering in this paper, a hull can be defined as a subset of all terms such that if term $\prod_{j \in J} k_j(\bf x, x')$ is included in the subset, then so are all terms $\prod_{j \in J / i} k_j(\bf x, x')$, for all $i \in J$.  For details, see \cite{DBLP:journals/corr/abs-0909-0844}.}
Given each dimension's kernel, and a pre-defined weighting over all terms, HKL performs model selection by searching over hulls of interaction terms.
% \subsubsection{All-subsets kernel with uniform weightings}
In \cite{DBLP:journals/corr/abs-0909-0844}, Bach also fixes the relative weighting between orders of interaction with a single term $\alpha$, computing the sum over all orders by:
\begin{equation}
\label{eqn:uniform}
k_{a}({\bf x, x'}) = v_D^2 \prod_{d=1}^D \left(1 + \alpha k_{d}(x_{d}, x_{d}') \right)
\end{equation}
which has computational complexity $O(D)$.  However, this formulation forces the weight of all $n$th order terms to be weighted by $\alpha^n$.

The main difficulty with the approach of \cite{DBLP:journals/corr/abs-0909-0844} is that hyperparameters are hard to set other than by cross-validation.  In contrast, our method optimizes the hyperparameters of each dimension's base kernel, as well as the relative weighting of each order of interaction. 

Accuracy versus interpretability in flexible modeling: Implementing a tradeoff using Gaussian process models \cite{plate1999accuracy}

A related functional ANOVA GP model\cite{kaufman2010bayesian} decomposes the \emph{mean} function into a weighted sum of GPs. However, the effect of a particular degree of interaction cannot be quantified by that approach. Also, computationally, the Gibbs sampling approach used in \cite{kaufman2010bayesian} is disadvantageous.

\subsection{Genetic Searches}

Evolving kernel functions for SVMs by genetic programming: \cite{diosan2007evolving}

A Genetic Programming based kernel construction and optimization method for Relevance Vector Machines: \cite{bing2010gp}

\subsection{Equation Learning}

Equation discovery with ecological applications \cite{dzeroski1999equation}

Discovering admissible model equations from observed data based on scale-types and identity constraints \cite{washio1999discovering}

\subsection{Multiple Kernel Learning}

\cite{christoudias2009bayesian} previously showed how mixtures of kernels can be learnt by gradient descent in the Gaussian process framework.  They call this \emph{Bayesian localized multiple kernel learning}.


\section{Experiments}

\subsection{Kernels used}

\input{"tables/kernel_descriptions.tex"}
Table \ref{tbl:kernel_descriptions} describes the various kernels used in our search.


%\subsection{Real datasets}

\subsubsection{Mauna Loa Atmoshperic Carbon Dioxide}

As an example of a GP modeling problem where choosing an appropriate structure is critical, we revisit a dataset explored in \cite{rasmussen38gaussian}, pages 120-126, where a kernel was hand-tailored to fit a GP model to the dataset.

%\begin{figure}
%\includegraphics[width=\columnwidth]{../figures/carls_kernel}
%\caption{Carl's kernel function on the Mauna dataset.}
%\end{figure}

%\begin{figure}
%\includegraphics[width=\columnwidth]{../figures/our_kernel}
%\caption{Our best kernel function on the Mauna dataset.}
%\end{figure}

%\begin{figure}[h]
%\includegraphics[width=\columnwidth]{../figures/carls_kernel_draw1}
%\caption{A draw from Carl's kernel.}
%\end{figure}

%\begin{figure}[h]
%\includegraphics[width=\columnwidth]{../figures/carls_kernel_draw1}
%\caption{Another draw from Carl's kernel.}
%\end{figure}

%\begin{figure}[h]
%\includegraphics[width=\columnwidth]{../figures/mauna_prior_draw_best_cov_v3}
%\caption{Another draw from our kernel.}
%\end{figure}

%\begin{figure}[h]
%\includegraphics[width=\columnwidth]{../figures/mauna_prior_draw_best_cov_v3}
%\caption{Another draw from our kernel.}
%\end{figure}

\subsection{Decomposition}

A benefit of the GP framework is that, when a function modeled with additive structure, we can decompose the signal into a sum of functions, and examine the individual components of our signal.

\PROBLEM{Missing links to figures - commented out for the moment}

%\begin{figure}
%\includegraphics[width=\columnwidth]{../figures/decomposition/mauna_test_all}
%\caption{The complete posterior on the Mauna dataset.}
%\end{figure}
%
%\begin{figure}
%\includegraphics[width=\columnwidth]{../figures/decomposition/mauna_test_1}
%\caption{The long-term smooth trend component of the Mauna posterior, corresponding to a SE kernel with a long lengthscale.}
%\end{figure}
%
%\begin{figure}
%\includegraphics[width=\columnwidth]{../figures/decomposition/mauna_test_2_zoom}
%\caption{The periodic, yearly component, corresponding to a SE kernel with a long lengthscale multiplied by a periodic kernel with a yearly period.}
%\end{figure}
%
%\begin{figure}
%\includegraphics[width=\columnwidth]{../figures/decomposition/mauna_test_3}
%\caption{The medium-term anomaly component of the Mauna posterior, corresponding to an SE kernel with a lengthscale of several years.}
%\end{figure}
%
%\begin{figure}
%\includegraphics[width=\columnwidth]{../figures/decomposition/mauna_test_4}
%\caption{The noise component of the Mauna posterior, corresponding to an SE kernel with a very short lengthscale.}
%\end{figure}

\subsubsection{Load forecasting}

Similar to the above, we apply the methodology to another highly structured time series, but with additional input data.
We analyse hourly load data for a US utility; this data was recently the focus of a data mining competition; the load forecasting track of GEFCom2012\footnotemark.
The data consists of hourly load measurements of a US utility over several years, split into 20 geographical zones and 11 temperature time series.
The relationships (geographical or otherwise) between the zones and the temperature stations were unknown for the purposes of the competition.

\footnotetext{\texttt{http://www.gefcom.org/}}
\begin{figure}
\includegraphics[width=\columnwidth]{../figures/gef_load_z01_t09_500}
\caption{Zone 1 and temperature station 9 from GEFCom2012 load forecasting data.}
\label{fig:gef_z01_t09}
\end{figure}

Figure~\ref{fig:gef_z01_t09} shows the first 500 data points (approx.~20 days) of zone 1 together with temperature station 9 (all data has been standardised to have zero mean and unit standard deviation).
The plot shows that the load data follows a smooth trend with near periodic deviations.
The overall trend is somehow related to the temperature and some of the spikes in the load data appear to be related to spikes in temperature.

We applied our kernel selection methodology to this data set (using all temperature stations).
The kernels discovered at subsequent levels of the search were:
\begin{enumerate}
\item \texttt{RQ\_t(-1.8,  0.0, -1.0)}
\item \texttt{RQ\_t(-1.4,  0.0, -2.1) + PE\_t(-0.7,  0.0, -0.8)}
\item \texttt{RQ\_t( 0.3, -0.1, -1.9) * ( PE\_t(-0.9,  0.0, -1.0) + RQ\_t(-0.6,  0.0, -2.2) )}
\item \texttt{RQ\_t( 0.3, -0.1, -2.1) * SE\_T9( 1.2,  0.0) * ( PE\_t(-0.8,  0.0, -0.9) + RQ\_t(-0.5, -0.1, -2.2) )}
\end{enumerate}
The learnt kernels can be interpreted as follows.
The simplest model explains the data as a smooth function of time, with a short lengthscale to account for all of the variation.
The next model explains the data as a sum of a smooth component and periodic component with a period of one day; the lengthscale of the smooth component has increased since some of the fine detail can be explained by the periodic component.
The next model also splits the function into smooth and periodic components, but the periodic component is multiplied by a rational quadratic kernel which allows it to vary smoothly through time; again the lengthscale of the smooth component has increased since the fine detail has been better explained by the periodic component.
Finally, the model introduces a temperature variable to further explain the observed data.

One can visualise how the different components of the kernel capture the structure of the data.
Figure~\ref{fig:gef_z01_two_means} shows the components of the Gaussian process posterior mean corresponding to the additive components of the kernel function (after expanding brackets and grouping product terms).

\begin{figure}
\includegraphics[width=\columnwidth]{../figures/gef_load_z01_500_posteriors}
\caption{Zone 1 from GEFCom 2012 separated into smooth and periodic component}
\label{fig:gef_z01_two_means}
\end{figure}

\TBD{Not sure the below is going to work.}
These posterior means have been evaluated at a particular 1 dimensional trace through time and temperature.
We can however examine the posterior mean of the Gaussian process at other points in time and temperature space to better understand the interaction.
\TBD{If we plot the smooth component, it would be super awesome if we could see the weekend or other interpretable blips in time, whilst most of the variation in the trend might be explained by temperature. Kernel might not be complicated enough to really show this though \frownie.}

\TBD{What else can we say about this data? Numerical results might take a while to produce.}

\subsection{Synthetic experiments}

\subsubsection{Bach Synthetic Dataset}
In addition to standard UCI repository datasets, we generated a synthetic dataset following the same recipe as \cite{DBLP:journals/corr/abs-0909-0844}: From a covariance matrix drawn from a Wishart distribution with 1024 degrees of freedom, we select 8 variables.  We then construct the non-linear function $f(X) = \sum_{i=1}^4 \sum_{j=1+1}^4 X_i X_j + \epsilon$, which sums all 2-way products of the first 4 variables, and adds Gaussian noise $\epsilon$.  This dataset is one which can be predicted well by a kernel which is a sum of two-way interactions over the first 4 variables, ignoring the extra 4 noisy copies.

This dataset was designed by \cite{DBLP:journals/corr/abs-0909-0844} to demonstrate the advantages of HKL over GP-ARD. 

If the dataset is large enough, HKL can construct a hull around only those subsets of cross terms that are optimal for predicting the output.  GP-ARD, in contrast, can only learn to ignore the noisy copy variables, but cannot learn to ignore the higher-term interactions between the predictive variables.  However, a GP with an additive kernel can learn both to ignore irrelevant variables, and to ignore certain orders of interaction.  In this example, the additive GP is able to recover the relevant structure.

\subsection{Methods}

\subsubsection{Our method}
All of the experiments in this paper were performed using the standard GPML toolbox\footnote{Available at \texttt{http://www.gaussianprocess.org/gpml/code/}}; code to perform all experiments is available at the authors' website.

\subsubsection{Hierarchical Kernel Learning}	
HKL\footnote{Code for HKL available at \texttt{http://www.di.ens.fr/\textasciitilde fbach/hkl/}} was run using the all-subsets kernel, which corresponds to the same set of kernels as considered by the additive GP with a squared-exp base kernel.

\input{"tables/kernels.tex"}
\input{"tables/kernels2.tex"}
\input{"tables/kernels3.tex"}
\input{"tables/kernels4.tex"}

\subsection{Comparison to additive kernels table}

Some results are in - how do they do?
9 Jan results had some errors - read numbers with caution - k=3, depth=4, BIC.
18 Jan results are currently not known to be buggy - k=3, depth=4, BIC.

Results are not terrible but there is certainly room for improvement against additive kernels.
Some thoughts: surprising that Sq Exp is often better than HKL and GP GAM - are the experiments in a strange regime of some sort?
I will try running the experiments deeper with only Laplace approx.
This should be able to include additive kernels and will indirectly test the Laplace approx.

\begin{table*}[h!]
\begin{center}
\begin{tabular}{l | r r r r r}
Method & bach & concrete & pumadyn-8nh & servo & housing \\
\hline
Linear regression & & & & & \\
GP GAM & & & & & \\
HKL & 0.199 & 0.147 & 0.346 & 0.199 & 0.151 \\
Sq-exp & 0.045 & 0.157 & 0.317 & 0.126 & 0.092 \\
Additive & 0.045 & 0.089 & 0.316 & 0.110 & 0.102 \\
\hline
9 Jan & 0.300 & 0.117 & 0.318 & 0.127 & 0.115 \\
18 Jan & 0.211 & 0.121 & 0.318 & 0.134 & 0.172\\
\end{tabular}
\caption{MSE}
\end{center}
\end{table*}

\begin{table*}[h!]
\begin{center}
\begin{tabular}{l | r r r r r}
Method & bach & concrete & pumadyn-8nh & servo & housing \\
\hline
Linear regression & & & & & \\
GP GAM & & & & & \\
Sq-exp & -0.131 & 0.398 & 0.843 & 0.429 & 0.207 \\
Additive & -0.131 & 0.114 & 0.841 & 0.309 & 0.194 \\
\hline
9 Jan & 0.540 & 0.291 & 0.846 & 0.372 & 0.112 \\
18 Jan & 0.331 & 0.355 & 0.846 & 0.314 & 0.249\\
\end{tabular}
\caption{Mean neg log lik}
\end{center}
\end{table*}

\section{Discussion}

Machine learning can be more data-driven, analogous to the high-thoughput approaches being used in biology. 

\section{Future Work}
\subsection{Bayesian Optimization}

cite Ryan Adams, Lizotte, de Frietas

\subsection{A Kernel between kernels}

Hyperkernels \cite{ong2002hyperkernels}
\section{Model-based search over models}

Bayesian optimization for hyper-parameter search: \cite{snoek2012practical}


\section{Conclusion}

\subsubsection*{Acknowledgements}


\bibliographystyle{format/icml2013}
\bibliography{gpss}
\end{document}
